---
title: Big Data Analytics using Spark
tags: [spark]
style: fill
color: warning
description : 
---

#### Prequisites

Install jupyter and spark. Preferaby using the Docker.

#### Welcome

This post contain materials about how to perform statistical analysis of very large datasets that do not fit on a single computer. We will learn some of the most popular tools for performing this type of analysis: apache spark, XGBoost and TensorFlow. We will learn how to use these tools through Jupyter Notebooks and experience the power of combining narrative, code and graphics to create convincing analytical documents.

We have two main goals: 
1. The first is introduction to using large scale data analysis frameworks (Spark, XGBoost and TensorFlow). This includes the underlying computer architecture and the programming abstractions. 
2. The second is to combine methods from statistics and machine learning to perform large scale analysis, identify statistically significant pattern and visualize statistical summaries.

Topics:
- Memory Hierarchy, latency vs. throughput.
- Spark Basics
- Dataframes and SQL
- PCA and weather analysis
- K-means and intrinsic dimensions
- Decision trees, boosting, and random forests
- Neural Networks and TensorFlow

##### Setup using Docker

Download [Docker](https://store.docker.com/search?type=edition&offering=community)
